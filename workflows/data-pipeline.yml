name: Data Pipeline CI/CD

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'data/**'
      - 'pipelines/**'
      - 'etl/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'data/**'
      - 'pipelines/**'
      - 'etl/**'
  schedule:
    - cron: '0 2 * * *'  # Daily data quality checks at 2 AM

jobs:
  data-quality:
    name: Data Quality Checks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy scipy scikit-learn great-expectations
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run data validation
      run: |
        python -m pytest tests/data_validation/ -v --tb=short
      continue-on-error: false

    - name: Check data schemas
      run: |
        python scripts/validate_schemas.py

    - name: Generate data quality report
      if: always()
      run: |
        python scripts/generate_dq_report.py

    - name: Upload quality report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: data-quality-report
        path: reports/data_quality_*.html

  etl-test:
    name: ETL Pipeline Tests
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install apache-airflow pytest pytest-cov
        if [ -f etl/requirements.txt ]; then pip install -r etl/requirements.txt; fi

    - name: Run ETL unit tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost/test_db
      run: |
        pytest etl/tests/ -v --cov=etl --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: etl
        name: etl-coverage

  analytics-validation:
    name: Analytics Model Validation
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install ML dependencies
      run: |
        python -m pip install --upgrade pip
        pip install tensorflow scikit-learn pandas numpy matplotlib seaborn
        if [ -f analytics/requirements.txt ]; then pip install -r analytics/requirements.txt; fi

    - name: Validate model performance
      run: |
        python scripts/validate_models.py --threshold 0.85

    - name: Check for data drift
      run: |
        python scripts/detect_drift.py --sensitivity medium

    - name: Generate model report
      run: |
        python scripts/generate_model_report.py

    - name: Upload model metrics
      uses: actions/upload-artifact@v3
      with:
        name: model-metrics
        path: reports/model_metrics_*.json

  security-scan:
    name: Security & Compliance
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Run HIPAA compliance check
      run: |
        python scripts/hipaa_compliance_check.py

    - name: Scan for sensitive data
      run: |
        python scripts/scan_sensitive_data.py

    - name: Check encryption standards
      run: |
        python scripts/verify_encryption.py

    - name: Run Trivy security scan
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload security results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  deploy-staging:
    name: Deploy to Staging
    needs: [data-quality, etl-test, analytics-validation, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'

    steps:
    - uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Deploy data pipelines
      run: |
        echo "Deploying to staging environment..."
        # Add actual deployment commands here

    - name: Run smoke tests
      run: |
        python scripts/smoke_tests.py --env staging